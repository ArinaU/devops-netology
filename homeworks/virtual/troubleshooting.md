# Домашнее задание к занятию "6.6. Troubleshooting"

https://github.com/netology-code/virt-homeworks/tree/master/06-db-06-troobleshooting

## Задача 1 - MongoDB

* Список операций, которые надо производить для остановки запроса пользователя:
1) Узнать у юзера id операции с помощью 
``` db.currentOp( { "$ownOps": true } ) ```

2) Остановить данную операцию, передав id: ``` db.killOp(<opId>) ```

* Вариант решения проблемы с долгими (зависающими) запросами в MongoDB:

    Можно задать в команде максимальное время выполнения операции: 
    
    ``` db.runCommand( { distinct: "collection", key: "city", maxTimeMS: 45 } ) ```


## Задача 2 - Redis

Скорее всего это из-за функции WAIT в Redis, потому что как раз эта команда блокирует клиента до тех пор, пока все предыдущие команды записи не будут успешно переданы и подтверждены, по крайней мере, указанным числом реплик, пока не истечет время ожидания.
Как раз наверное сначала было небольшое количество записей, что надо было гарантировать предыдущие записи, поэтому наблюдался рост отношения записанных значений к истекшим.

Стоит использовать команду как часть транзакции MULTI, так как тогда блокировки нет, вместо этого просто возвращается как можно скорее количество реплик, подтвердивших предыдущие команды записи.

## Задача 3 - MySQL

Если есть такая ошибка, то могут быть следующие причины:
* Обычно это может указывать на проблемы с подключением к сети, следует проверить состояние сети, если эта ошибка возникает часто.
* Форма «during query» возникает, когда миллионы строк отправляются как часть одного или нескольких запросов. Если при проверке (просто посмотреть кол-во возвращаемых объектов при таком запросе или explain/explain analyze) оказалось, что действительно запросы к бд включают миллионы строк, то следует попробовать увеличить net_read_timeout со значения по умолчанию 30 секунд до 60 секунд или дольше, что достаточно для завершения передачи данных.
* Реже это происходит, когда клиент пытается установить начальное соединение с сервером. В этом случае, если для параметра connect_timeout установлено значение всего несколько секунд, то можно увеличить его до 10 секунд или больше, если очень большое расстояние передачи данных или медленное соединение. Определить можно, используя SHOW GLOBAL STATUS LIKE 'Aborted_connects'.
* Однако мне кажется более вероятной причина того, что для хранения изображений для гис-системы в БД используется BLOB тип, поэтому может возникнуть проблема со значениями BLOB, превышающими max_allowed_packet, что может вызвать эту ошибку у некоторых клиентов. Если иногда еще есть ошибка ER_NET_PACKET_TOO_LARGE, то это подтверждает, что нужно увеличить max_allowed_packet.


## Задача 4 - PostgreSQL

Out-Of-Memory Killer - это процесс, который отвечает за завершение работы приложения, чтобы спасти ядро от сбоя, поскольку он только убивает приложение и спасает всю ОС. Очевидно, что когда out_of_memory возникает несколько раз, это происходит из-за ожидания ввода-вывода или из-за ожидания страницы для swap-а на диск.

Также, возможно БД имеет много текущих соеднинений, и БД вынуждена хранить кэш метаданных для каждого соединения обо всех объектах базы данных (таблицах, индексах и т.д.), которые были затронуты в течение времени существования соединения. И так как нет верхнего предела того, насколько большим может быть этот кэш, и нет механизма его истечения, появляется данная ошибка Out of memory.

Можно задать данный предел с помощью idle_in_transaction_session_timeout(integer), что завершит любой сеанс открытой транзакции дольше указанного времени в миллисекундах. Это позволяет снять любые блокировки, удерживаемые этим сеансом, и повторно использовать слот подключения; он также позволяет очищать кортежи, видимые только для этой транзакции.
